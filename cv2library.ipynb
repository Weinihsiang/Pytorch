{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXlXUl7Pam48nh4aupCSiY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weinihsiang/Pytorch/blob/main/cv2library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjWpt1mtnQm7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def resize_nearest_neighbor(image, new_size):\n",
        "    old_height, old_width = image.shape[:2]\n",
        "    new_height, new_width = new_size\n",
        "\n",
        "    # Calculate scaling ratios\n",
        "    height_ratio = old_height / new_height\n",
        "    width_ratio = old_width / new_width\n",
        "\n",
        "    # Generate coordinates for new image pixels\n",
        "    new_y = np.arange(new_height) * height_ratio\n",
        "    new_x = np.arange(new_width) * width_ratio\n",
        "\n",
        "    # Round the coordinates to the nearest integer\n",
        "    new_y = np.round(new_y).astype(int)\n",
        "    new_x = np.round(new_x).astype(int)\n",
        "\n",
        "    # Ensure that the coordinates are within the image bounds\n",
        "    new_y = np.clip(new_y, 0, old_height - 1)\n",
        "    new_x = np.clip(new_x, 0, old_width - 1)\n",
        "\n",
        "    # Reshape coordinates for indexing\n",
        "    new_y = new_y.reshape((new_height, 1))\n",
        "    new_x = new_x.reshape((1, new_width))\n",
        "\n",
        "    # Index the original image with the new coordinates\n",
        "    resized_image = image[new_y, new_x]\n",
        "\n",
        "    return resized_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nearest neighbor"
      ],
      "metadata": {
        "id": "gd10eMMahWO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def resize_bilinear(image, new_size):\n",
        "    old_height, old_width = image.shape[:2]\n",
        "    new_height, new_width = new_size\n",
        "\n",
        "    # Calculate scaling ratios\n",
        "    height_ratio = old_height / new_height\n",
        "    width_ratio = old_width / new_width\n",
        "\n",
        "    # Generate coordinates for new image pixels\n",
        "    y = np.arange(new_height) * height_ratio\n",
        "    x = np.arange(new_width) * width_ratio\n",
        "\n",
        "    # Calculate the indices of the four nearest pixels for each new coordinate\n",
        "    y_floor = np.floor(y).astype(int)\n",
        "    x_floor = np.floor(x).astype(int)\n",
        "    y_ceil = np.clip(np.ceil(y).astype(int), 0, old_height - 1)\n",
        "    x_ceil = np.clip(np.ceil(x).astype(int), 0, old_width - 1)\n",
        "\n",
        "    # Calculate the fractional part of each coordinate\n",
        "    y_fraction = y - y_floor\n",
        "    x_fraction = x - x_floor\n",
        "\n",
        "    # Interpolate values for each channel using bilinear interpolation\n",
        "    interpolated = np.zeros((new_height, new_width, image.shape[2]), dtype=np.uint8)\n",
        "    for channel in range(image.shape[2]):\n",
        "        top_left = image[y_floor, x_floor, channel]\n",
        "        top_right = image[y_floor, x_ceil, channel]\n",
        "        bottom_left = image[y_ceil, x_floor, channel]\n",
        "        bottom_right = image[y_ceil, x_ceil, channel]\n",
        "\n",
        "        interpolated[:, :, channel] = (1 - y_fraction) * ((1 - x_fraction) * top_left + x_fraction * top_right) + \\\n",
        "                                       y_fraction * ((1 - x_fraction) * bottom_left + x_fraction * bottom_right)\n",
        "\n",
        "    return interpolated\n"
      ],
      "metadata": {
        "id": "c2o-4okrhUzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bilinear method"
      ],
      "metadata": {
        "id": "-zfm3l15hVZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def resize_bicubic(image, new_size):\n",
        "    old_height, old_width = image.shape[:2]\n",
        "    new_height, new_width = new_size\n",
        "\n",
        "    # Calculate scaling ratios\n",
        "    height_ratio = old_height / new_height\n",
        "    width_ratio = old_width / new_width\n",
        "\n",
        "    # Generate coordinates for new image pixels\n",
        "    y = np.arange(new_height) * height_ratio\n",
        "    x = np.arange(new_width) * width_ratio\n",
        "\n",
        "    # Calculate the indices and fractional parts of the coordinates\n",
        "    y_floor = np.floor(y).astype(int)\n",
        "    x_floor = np.floor(x).astype(int)\n",
        "    y_fraction = y - y_floor\n",
        "    x_fraction = x - x_floor\n",
        "\n",
        "    # Apply bicubic interpolation to each channel\n",
        "    interpolated = np.zeros((new_height, new_width, image.shape[2]), dtype=np.uint8)\n",
        "    for channel in range(image.shape[2]):\n",
        "        for i in range(new_height):\n",
        "            for j in range(new_width):\n",
        "                i_floor = y_floor[i]\n",
        "                j_floor = x_floor[j]\n",
        "\n",
        "                p = np.zeros((4, 4))\n",
        "                for m in range(-1, 3):\n",
        "                    for n in range(-1, 3):\n",
        "                        p[m+1, n+1] = image[\n",
        "                            np.clip(i_floor + m, 0, old_height - 1),\n",
        "                            np.clip(j_floor + n, 0, old_width - 1),\n",
        "                            channel\n",
        "                        ]\n",
        "\n",
        "                interpolated[i, j, channel] = bicubic_interpolation(p, y_fraction[i], x_fraction[j])\n",
        "\n",
        "    return interpolated\n",
        "\n",
        "\n",
        "def bicubic_interpolation(p, y_fraction, x_fraction):\n",
        "    a = np.array([\n",
        "        [-0.5, 1.5, -1.5, 0.5],\n",
        "        [1, -2.5, 2, -0.5],\n",
        "        [-0.5, 0, 0.5, 0],\n",
        "        [0, 1, 0, 0]\n",
        "    ])\n",
        "\n",
        "    fractional_matrix = np.array([1, x_fraction, x_fraction ** 2, x_fraction ** 3])\n",
        "    interpolated_value = np.dot(fractional_matrix, np.dot(a, np.dot(p, a.T)))\n",
        "\n",
        "    return np.clip(interpolated_value, 0, 255).astype(np.uint8)\n"
      ],
      "metadata": {
        "id": "alCuc972hh-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cubic interpolation"
      ],
      "metadata": {
        "id": "ug3Eo3eihk0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def lanczos_kernel(x, a):\n",
        "    if x == 0:\n",
        "        return 1.0\n",
        "    elif -a < x < a:\n",
        "        return a * np.sin(np.pi * x) * np.sin(np.pi * x / a) / (np.pi * np.pi * x * x)\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "def resize_lanczos4(image, new_size):\n",
        "    new_height, new_width = new_size\n",
        "    height, width = image.shape[:2]\n",
        "    resized = np.zeros((new_height, new_width, image.shape[2]), dtype=np.uint8)\n",
        "\n",
        "    y_ratio = height / new_height\n",
        "    x_ratio = width / new_width\n",
        "\n",
        "    for i in range(new_height):\n",
        "        for j in range(new_width):\n",
        "            y = (i + 0.5) * y_ratio\n",
        "            x = (j + 0.5) * x_ratio\n",
        "\n",
        "            y_floor = int(y)\n",
        "            x_floor = int(x)\n",
        "            a = 3\n",
        "\n",
        "            result = np.zeros(image.shape[2], dtype=np.float32)\n",
        "            for m in range(-a + 1, a):\n",
        "                for n in range(-a + 1, a):\n",
        "                    y_fraction = y - y_floor\n",
        "                    x_fraction = x - x_floor\n",
        "                    weight = lanczos_kernel(y_fraction - m, a) * lanczos_kernel(x_fraction - n, a)\n",
        "                    if 0 <= y_floor + m < height and 0 <= x_floor + n < width:\n",
        "                        result += weight * image[y_floor + m, x_floor + n]\n",
        "\n",
        "            resized[i, j] = np.clip(result, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return resized\n"
      ],
      "metadata": {
        "id": "xAmjMT58hm92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lanczos interpolation with a kernel size of 4 using NumPy"
      ],
      "metadata": {
        "id": "I5Lys5odhpeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "# Upload the image file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded image file name\n",
        "image_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Load the image\n",
        "image = Image.open(image_filename)\n",
        "\n",
        "# Convert the image to NumPy array\n",
        "image_array = np.array(image)\n",
        "\n",
        "# Resize the image using custom interpolation methods\n",
        "resized_nearest = resize_nearest_neighbor(image_array, (2000, 3000))\n",
        "resized_bilinear = resize_bilinear(image_array, (2000, 3000))\n",
        "resized_bicubic = resize_bicubic(image_array, (2000, 3000))\n",
        "resized_lanczos = resize_lanczos4(image_array, (2000, 3000))\n",
        "\n",
        "# Convert the resized images back to PIL Image\n",
        "resized_nearest = Image.fromarray(resized_nearest)\n",
        "resized_bilinear = Image.fromarray(resized_bilinear)\n",
        "resized_bicubic = Image.fromarray(resized_bicubic)\n",
        "resized_lanczos = Image.fromarray(resized_lanczos)\n",
        "\n",
        "# Display or save the resized images as desired\n",
        "resized_nearest.show()\n",
        "resized_bilinear.show()\n",
        "resized_bicubic.show()\n",
        "resized_lanczos.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "I8ZdXqK2iN8D",
        "outputId": "93e80055-ea6b-4797-a418-912e35085874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-62148d4b-2ff8-416a-bcbf-72203cbc195f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-62148d4b-2ff8-416a-bcbf-72203cbc195f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving piano.png to piano.png\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7a3dd3c390cf>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Convert the image to NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mimage_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Resize the image using custom interpolation methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = int(input(\"How many nodes are there? \"))\n",
        "number = [[0] * (nodes+1) for _ in range(nodes+1)]\n",
        "numberexist = [0]*(nodes+1)\n",
        "height = [0]*(nodes+1)\n",
        "max = [-1]*(nodes+1)\n",
        "\n",
        "for i in range(nodes):\n",
        "    list = input().split()\n",
        "    for j in range(int(list[0])):\n",
        "        number[i+1][j] = int(list[j+1])\n",
        "        numberexist[number[i+1][j]]+=1\n",
        "print(number)\n",
        "for k in range(nodes+1):\n",
        "    for meow in range(nodes+1):\n",
        "        l = 0\n",
        "        while(number[meow][l]!=0):\n",
        "            if height[int(number[meow][l])]>=max[meow]:\n",
        "                max[meow] = height[int(number[meow][l])]\n",
        "                height[meow] = height[int(number[meow][l])]+1\n",
        "            l +=1\n",
        "    if numberexist[k] ==0 and k != 0:\n",
        "        print(k)\n",
        "print(height)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uptc3bQk7r8b",
        "outputId": "d6ac72bb-ae57-4723-be99-da03951709b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many nodes are there? 9\n",
            "1 6\n",
            "3 5 3 8\n",
            "0\n",
            "2 1 7\n",
            "1 9\n",
            "0\n",
            "1 2\n",
            "0\n",
            "0\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 3, 8, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 7, 0, 0, 0, 0, 0, 0, 0, 0], [9, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "4\n",
            "[0, 1, 2, 0, 4, 1, 0, 3, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pycraigslist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbqECFw_eGAG",
        "outputId": "e5a43299-3f30-4913-d08b-61ce1b14cd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycraigslist\n",
            "  Downloading pycraigslist-0.7.1-py3-none-any.whl (28 kB)\n",
            "Collecting httpx>=0.18.0 (from pycraigslist)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from pycraigslist) (4.11.2)\n",
            "Requirement already satisfied: tenacity>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from pycraigslist) (8.2.3)\n",
            "Collecting cchardet>=2.1.0 (from pycraigslist)\n",
            "  Downloading cchardet-2.1.7.tar.gz (653 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from pycraigslist) (4.9.3)\n",
            "Collecting fake_headers>=1.0.2 (from pycraigslist)\n",
            "  Downloading fake_headers-1.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9.0->pycraigslist) (2.4.1)\n",
            "Collecting bs4 (from fake_headers>=1.0.2->pycraigslist)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from fake_headers>=1.0.2->pycraigslist) (1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.18.0->pycraigslist) (2023.7.22)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx>=0.18.0->pycraigslist)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.18.0->pycraigslist) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.18.0->pycraigslist) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx>=0.18.0->pycraigslist)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx>=0.18.0->pycraigslist) (3.7.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->fake_headers>=1.0.2->pycraigslist) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->fake_headers>=1.0.2->pycraigslist) (0.5.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx>=0.18.0->pycraigslist) (1.1.3)\n",
            "Building wheels for collected packages: cchardet, bs4\n",
            "  Building wheel for cchardet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cchardet: filename=cchardet-2.1.7-cp310-cp310-linux_x86_64.whl size=249355 sha256=f9611349ef7f8173d5d157ee5eb4a14131b2cfda7bf5fde370865642770eb34f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/e0/ab/e01326f15c59438d080b1496dbab8091e952ec72f35e3c437e\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=dac901cbb5b6ee756107b19c706e8e347ed221a08a6ab4c2be0187b5617bbce2\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built cchardet bs4\n",
            "Installing collected packages: cchardet, h11, httpcore, bs4, httpx, fake_headers, pycraigslist\n",
            "Successfully installed bs4-0.0.1 cchardet-2.1.7 fake_headers-1.0.2 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 pycraigslist-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycraigslist\n",
        "\n",
        "print(pycraigslist.housing.get_categories())"
      ],
      "metadata": {
        "id": "iR3nPr0JiOgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6e2b55-ab29-40c9-c45e-02a541b8e376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'apa': 'apartments / housing for rent', 'swp': 'housing swap', 'off': 'office & commercial', 'prk': 'parking & storage', 'rea': 'real estate', 'reb': 'real estate - by dealer', 'reo': 'real estate - by owner', 'roo': 'rooms & shares', 'sub': 'sublets & temporary', 'vac': 'vacation rentals', 'hou': 'wanted: apts', 'rew': 'wanted: real estate', 'sha': 'wanted: room/share', 'sbw': 'wanted: sublet/temp'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"APPLEPPLE\"\n",
        "word[::-1]"
      ],
      "metadata": {
        "id": "i3gvKdXttH6U",
        "outputId": "8cf5c23a-af93-4438-99b8-a32835f0294b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ELPPELPPA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class EbaySpider(scrapy.Spider):\n",
        "\n",
        "\tname = \"ebay\"\n",
        "\tallowed_domains = [\"ebay.com\"]\n",
        "\tstart_urls = [\"https://www.ebay.com\"]\n",
        "\n",
        "\t# Allow a custom parameter (-a flag in the scrapy command)\n",
        "\tdef __init__(self, search=\"nintendo switch console\"):\n",
        "\t\tself.search_string = search\n",
        "\n",
        "\tdef parse(self, response):\n",
        "\t\t# Extrach the trksid to build a search request\n",
        "\t\ttrksid = response.css(\"input[type='hidden'][name='_trksid']\").xpath(\"@value\").extract()[0]\n",
        "\n",
        "\t\t# Build the url and start the requests\n",
        "\t\tyield scrapy.Request(\"http://www.ebay.com/sch/i.html?_from=R40&_trksid=\" + trksid +\n",
        "\t\t\t\t\t\t\t \"&_nkw=\" + self.search_string.replace(' ','+') + \"&_ipg=200\",\n",
        "\t\t\t\t\t\t\t callback=self.parse_link)\n",
        "\n",
        "\t\tprint('Hello Michael')\n",
        "\n",
        "\t# Parse the search results\n",
        "\tdef parse_link(self, response):\n",
        "\t\t# Extract the list of products\n",
        "\t\tresults = response.xpath('//div/div/ul/li[contains(@class, \"s-item\" )]')\n",
        "\n",
        "\t\t# Extract info for each product\n",
        "\t\tfor product in results:\n",
        "\t\t\tname = product.xpath('.//*[@class=\"s-item__title\"]//text()').extract_first()\n",
        "\t\t\t# Sponsored or New Listing links have a different class\n",
        "\t\t\tif name == None:\n",
        "\t\t\t\tname = product.xpath('.//*[@class=\"s-item__title s-item__title--has-tags\"]/text()').extract_first()\n",
        "\t\t\t\tif name == None:\n",
        "\t\t\t\t\tname = product.xpath('.//*[@class=\"s-item__title s-item__title--has-tags\"]//text()').extract_first()\n",
        "\t\t\tif name == 'New Listing':\n",
        "\t\t\t\tname = product.xpath('.//*[@class=\"s-item__title\"]//text()').extract()[1]\n",
        "\n",
        "\t\t\t# If this get a None result\n",
        "\t\t\tif name == None:\n",
        "\t\t\t\tname = \"ERROR\"\n",
        "\n",
        "\t\t\tprice = product.xpath('.//*[@class=\"s-item__price\"]/text()').extract_first()\n",
        "\t\t\tstatus = product.xpath('.//*[@class=\"SECONDARY_INFO\"]/text()').extract_first()\n",
        "\t\t\tseller_level = product.xpath('.//*[@class=\"s-item__etrs-text\"]/text()').extract_first()\n",
        "\t\t\tlocation = product.xpath('.//*[@class=\"s-item__location s-item__itemLocation\"]/text()').extract_first()\n",
        "\t\t\tproduct_url = product.xpath('.//a[@class=\"s-item__link\"]/@href').extract_first()\n",
        "\n",
        "\t\t\t# Set default values\n",
        "\t\t\tstars = 0\n",
        "\t\t\tratings = 0\n",
        "\n",
        "\t\t\tstars_text = product.xpath('.//*[@class=\"clipped\"]/text()').extract_first()\n",
        "\t\t\tif stars_text: stars = stars_text[:3]\n",
        "\t\t\tratings_text = product.xpath('.//*[@aria-hidden=\"true\"]/text()').extract_first()\n",
        "\t\t\tif ratings_text: ratings = ratings_text.split(' ')[0]\n",
        "\n",
        "\t\t\tsummary_data = {\n",
        "\t\t\t\t\t\t\t\"Name\":name,\n",
        "\t\t\t\t\t\t\t\"Status\":status,\n",
        "\t\t\t\t\t\t\t#\"Seller_Level\":seller_level,\n",
        "\t\t\t\t\t\t\t#\"Location\":location,\n",
        "\t\t\t\t\t\t\t\"Price\":price,\n",
        "\t\t\t\t\t\t\t\"Stars\":stars,\n",
        "\t\t\t\t\t\t\t\"Ratings\":ratings,\n",
        "\t\t\t\t\t\t\t\"URL\": product_url\n",
        "\t\t\t\t\t\t\t}\n",
        "\n",
        "\t\t\t# Go to the product details page\n",
        "\t\t\tdata = {'summary_data': summary_data}\n",
        "\t\t\tprint(data)\n",
        "\t\t\t# print(scrapy.Request(product_url, meta=data, callback=self.parse_product_details))\n",
        "\n",
        "\t\t# Get the next page\n",
        "\t\tnext_page_url = response.xpath('//*/a[@class=\"x-pagination__control\"][2]/@href').extract_first()\n",
        "\n",
        "\t\t# The last page do not have a valid url and ends with '#'\n",
        "\t\tif next_page_url == None or str(next_page_url).endswith(\"#\"):\n",
        "\t\t\tself.log(\"eBay products collected successfully !!!\")\n",
        "\t\telse:\n",
        "\t\t\tprint('\\n'+'-'*30)\n",
        "\t\t\tprint('Next page: {}'.format(next_page_url))\n",
        "\t\t\tyield scrapy.Request(next_page_url, callback=self.parse_link)\n",
        "\n",
        "\n",
        "\t# Parse details page for each product\n",
        "\tdef parse_product_details(self, response):\n",
        "\n",
        "\t\t# Get the summary data\n",
        "\t\tdata = response.meta['summary_data']\n",
        "\n",
        "\t\t# Add more data from details page\n",
        "\t\tdata['UPC'] = response.xpath('//h2[@itemprop=\"gtin13\"]/text()').extract_first()\n",
        "\n",
        "\t\tyield data\n",
        "\n"
      ],
      "metadata": {
        "id": "4SzGNJAegFcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "\n",
        "def run_spider():\n",
        "    process = CrawlerProcess(get_project_settings())\n",
        "    process.crawl(EbaySpider)  # Replace with your actual spider class\n",
        "    process.start()\n",
        "\n",
        "run_spider()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ao3PEw91guop",
        "outputId": "2d46fce1-e9bb-4556-d11f-deb6e0e106d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.10.0 started (bot: scrapybot)\n",
            "2023-08-26 18:23:15 [scrapy.utils.log] INFO: Scrapy 2.10.0 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-5.15.109+-x86_64-with-glibc2.35\n",
            "2023-08-26 18:23:15 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-5.15.109+-x86_64-with-glibc2.35\n",
            "INFO:scrapy.addons:Enabled addons:\n",
            "[]\n",
            "2023-08-26 18:23:15 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{}\n",
            "2023-08-26 18:23:15 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2023-08-26 18:23:15 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: 145c7bcd8e71a6a5\n",
            "2023-08-26 18:23:15 [scrapy.extensions.telnet] INFO: Telnet Password: 145c7bcd8e71a6a5\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-08-26 18:23:15 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-08-26 18:23:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-08-26 18:23:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "2023-08-26 18:23:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "INFO:scrapy.core.engine:Spider opened\n",
            "2023-08-26 18:23:15 [scrapy.core.engine] INFO: Spider opened\n",
            "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-08-26 18:23:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6029\n",
            "2023-08-26 18:23:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6029\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ReactorNotRestartable",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-54d807a714b9>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrun_spider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-54d807a714b9>\u001b[0m in \u001b[0;36mrun_spider\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_project_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEbaySpider\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your actual spider class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrun_spider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"REACTOR_THREADPOOL_MAXSIZE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"before\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shutdown\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \"\"\"\n\u001b[1;32m   1298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mReactorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReactorBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reallyStartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReactorNotRestartable\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1_OOXrv_0Nz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}